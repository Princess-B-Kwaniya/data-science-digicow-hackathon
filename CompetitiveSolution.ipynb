{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce2d169",
   "metadata": {},
   "source": [
    "# DigiCow Farmer Training Adoption Challenge\n",
    "## Competitive Solution v3 — Full Pipeline\n",
    "\n",
    "**Upgrades over v2:**\n",
    "1. Trainer/County/Ward/Topic adoption rate features (generalize to unseen farmers)\n",
    "2. Farmer adoption features computed PER CV FOLD (fixes CV leakage)\n",
    "3. Optuna hyperparameter tuning (20 trials)\n",
    "4. LightGBM + Logistic Regression ensemble\n",
    "5. Isotonic calibration for better Log Loss\n",
    "6. Stricter monotonicity enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f6f451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports OK\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Imports & Seed\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "TARGET_COLS = [\n",
    "    'adopted_within_07_days',\n",
    "    'adopted_within_90_days',\n",
    "    'adopted_within_120_days',\n",
    "]\n",
    "\n",
    "TARGET_TO_SUB = {\n",
    "    'adopted_within_07_days':  ('Target_07_AUC', 'Target_07_LogLoss'),\n",
    "    'adopted_within_90_days':  ('Target_90_AUC', 'Target_90_LogLoss'),\n",
    "    'adopted_within_120_days': ('Target_120_AUC', 'Target_120_LogLoss'),\n",
    "}\n",
    "\n",
    "print(\"All imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b61f6f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (16000, 17), Test: (6000, 14)\n",
      "Train dates: 2024-02-13 00:00:00 -> 2025-05-23 00:00:00\n",
      "Test  dates: 2025-05-26 00:00:00 -> 2026-01-19 00:00:00\n",
      "  adopted_within_07_days: pos_rate = 0.1565\n",
      "  adopted_within_90_days: pos_rate = 0.3469\n",
      "  adopted_within_120_days: pos_rate = 0.4477\n",
      "\n",
      "Farmer overlap: 1251 / 4225 test farmers seen in train\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Load Data\n",
    "# ============================================================\n",
    "train_df = pd.read_csv('Train.csv', parse_dates=['training_date'])\n",
    "test_df  = pd.read_csv('Test.csv',  parse_dates=['training_date'])\n",
    "\n",
    "print(f\"Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "print(f\"Train dates: {train_df['training_date'].min()} -> {train_df['training_date'].max()}\")\n",
    "print(f\"Test  dates: {test_df['training_date'].min()} -> {test_df['training_date'].max()}\")\n",
    "for t in TARGET_COLS:\n",
    "    print(f\"  {t}: pos_rate = {train_df[t].mean():.4f}\")\n",
    "\n",
    "# Check farmer overlap\n",
    "train_farmers = set(train_df['farmer_id'].unique())\n",
    "test_farmers = set(test_df['farmer_id'].unique())\n",
    "overlap = train_farmers & test_farmers\n",
    "print(f\"\\nFarmer overlap: {len(overlap)} / {len(test_farmers)} test farmers seen in train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28d9e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Feature Engineering — SAFE features only\n",
    "# ============================================================\n",
    "# These features do NOT depend on targets, so no CV leakage.\n",
    "# Target-based aggregates (adoption rates) are in a separate function\n",
    "# applied per-fold during CV.\n",
    "\n",
    "def build_base_features(df, train_ref):\n",
    "    \"\"\"Features that DON'T use target values — safe to compute once.\"\"\"\n",
    "    out = df.copy()\n",
    "    \n",
    "    # --- DATE FEATURES ---\n",
    "    out['training_month'] = out['training_date'].dt.month\n",
    "    out['training_dow'] = out['training_date'].dt.dayofweek\n",
    "    out['training_quarter'] = out['training_date'].dt.quarter\n",
    "    out['training_day_of_year'] = out['training_date'].dt.dayofyear\n",
    "    out['training_week'] = out['training_date'].dt.isocalendar().week.astype(int)\n",
    "    ref_date = pd.Timestamp('2024-01-01')\n",
    "    out['days_since_ref'] = (out['training_date'] - ref_date).dt.days\n",
    "    out['is_weekend'] = (out['training_dow'] >= 5).astype(int)\n",
    "    \n",
    "    # --- TOPIC CATEGORY ---\n",
    "    def categorize_topic(topic):\n",
    "        t = str(topic).lower()\n",
    "        if 'poultry' in t: return 'poultry'\n",
    "        elif any(w in t for w in ['dairy', 'milk', 'cow', 'cattle']): return 'dairy'\n",
    "        elif any(w in t for w in ['feed', 'nutrition', 'mineral']): return 'feeding'\n",
    "        elif any(w in t for w in ['health', 'disease', 'vaccin', 'deworm', 'antimicrobial']): return 'health'\n",
    "        elif any(w in t for w in ['record', 'app', 'digital', 'ndume']): return 'digital'\n",
    "        elif any(w in t for w in ['breed', 'ai ', 'infertil', 'reproduct']): return 'breeding'\n",
    "        elif any(w in t for w in ['market', 'business', 'profit', 'value']): return 'business'\n",
    "        elif any(w in t for w in ['fertiliz', 'soil', 'compost', 'organic']): return 'crop'\n",
    "        else: return 'other'\n",
    "    out['topic_category'] = out['topics'].apply(categorize_topic)\n",
    "    \n",
    "    # --- FARMER PRIOR TRAINING COUNT (vectorized, no target info) ---\n",
    "    train_sorted = train_ref[['farmer_id', 'training_date']].sort_values('training_date')\n",
    "    farmer_all_dates = train_sorted.groupby('farmer_id')['training_date'].apply(\n",
    "        lambda x: sorted(x.tolist())\n",
    "    ).to_dict()\n",
    "    \n",
    "    prior_counts = np.zeros(len(out), dtype=int)\n",
    "    days_since_last = np.full(len(out), -1, dtype=float)\n",
    "    \n",
    "    for fid, dates_list in farmer_all_dates.items():\n",
    "        dates_arr = np.array(dates_list, dtype='datetime64[ns]')\n",
    "        mask = out['farmer_id'] == fid\n",
    "        if mask.any():\n",
    "            row_dates = out.loc[mask, 'training_date'].values.astype('datetime64[ns]')\n",
    "            counts = np.searchsorted(dates_arr, row_dates, side='left')\n",
    "            prior_counts[mask.values] = counts\n",
    "            # Days since most recent prior training\n",
    "            for i, (idx, rd) in enumerate(zip(np.where(mask.values)[0], row_dates)):\n",
    "                c = counts[i]\n",
    "                if c > 0:\n",
    "                    last_date = dates_arr[c - 1]\n",
    "                    days_since_last[idx] = (rd - last_date) / np.timedelta64(1, 'D')\n",
    "    \n",
    "    out['farmer_prior_trainings'] = prior_counts\n",
    "    out['is_first_training'] = (prior_counts == 0).astype(int)\n",
    "    out['days_since_last_training'] = days_since_last\n",
    "    out['farmer_total_in_train'] = out['farmer_id'].map(\n",
    "        train_ref['farmer_id'].value_counts()\n",
    "    ).fillna(0).astype(int)\n",
    "    out['farmer_in_train'] = out['farmer_id'].isin(train_ref['farmer_id'].unique()).astype(int)\n",
    "    \n",
    "    # --- VOLUME / FREQUENCY (no target info) ---\n",
    "    for col, name in [('trainer', 'trainer'), ('group_name', 'group'), \n",
    "                      ('topics', 'topic'), ('county', 'county'),\n",
    "                      ('subcounty', 'subcounty'), ('ward', 'ward')]:\n",
    "        vol = train_ref[col].value_counts().to_dict()\n",
    "        out[f'{name}_volume'] = out[col].map(vol).fillna(0)\n",
    "    \n",
    "    topic_freq = train_ref['topics'].value_counts(normalize=True).to_dict()\n",
    "    out['topic_frequency'] = out['topics'].map(topic_freq).fillna(0)\n",
    "    \n",
    "    # --- INTERACTIONS ---\n",
    "    out['coop_x_has_topic'] = out['belong_to_cooperative'] * out['has_topic_trained_on']\n",
    "    \n",
    "    # --- GROUP-LEVEL (no target info) ---\n",
    "    group_coop = train_ref.groupby('group_name')['belong_to_cooperative'].mean().to_dict()\n",
    "    out['group_coop_rate'] = out['group_name'].map(group_coop).fillna(0.5)\n",
    "    group_size = train_ref.groupby('group_name')['farmer_id'].nunique().to_dict()\n",
    "    out['group_unique_farmers'] = out['group_name'].map(group_size).fillna(1)\n",
    "    \n",
    "    # --- TRAINER EXPERIENCE (# unique farmers, # unique topics — no target) ---\n",
    "    trainer_farmers = train_ref.groupby('trainer')['farmer_id'].nunique().to_dict()\n",
    "    out['trainer_unique_farmers'] = out['trainer'].map(trainer_farmers).fillna(0)\n",
    "    trainer_topics = train_ref.groupby('trainer')['topics'].nunique().to_dict()\n",
    "    out['trainer_unique_topics'] = out['trainer'].map(trainer_topics).fillna(0)\n",
    "    \n",
    "    # --- COUNTY-LEVEL STATS ---\n",
    "    county_farmers = train_ref.groupby('county')['farmer_id'].nunique().to_dict()\n",
    "    out['county_unique_farmers'] = out['county'].map(county_farmers).fillna(0)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def add_target_agg_features(df, train_fold, target_cols):\n",
    "    \"\"\"\n",
    "    Add adoption-rate features computed FROM train_fold only.\n",
    "    Called per-fold during CV to prevent leakage.\n",
    "    For final test predictions, train_fold = full train set.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    \n",
    "    for t in target_cols:\n",
    "        # Farmer adoption rates\n",
    "        farm_agg = train_fold.groupby('farmer_id')[t].agg(['mean', 'sum', 'count'])\n",
    "        out[f'farmer_{t}_rate'] = out['farmer_id'].map(farm_agg['mean']).fillna(-1)\n",
    "        out[f'farmer_{t}_sum'] = out['farmer_id'].map(farm_agg['sum']).fillna(-1)\n",
    "        out[f'farmer_{t}_count'] = out['farmer_id'].map(farm_agg['count']).fillna(0)\n",
    "        \n",
    "        # Trainer adoption rates\n",
    "        trainer_agg = train_fold.groupby('trainer')[t].mean().to_dict()\n",
    "        out[f'trainer_{t}_rate'] = out['trainer'].map(trainer_agg).fillna(train_fold[t].mean())\n",
    "        \n",
    "        # County adoption rates\n",
    "        county_agg = train_fold.groupby('county')[t].mean().to_dict()\n",
    "        out[f'county_{t}_rate'] = out['county'].map(county_agg).fillna(train_fold[t].mean())\n",
    "        \n",
    "        # Ward adoption rates  \n",
    "        ward_agg = train_fold.groupby('ward')[t].mean().to_dict()\n",
    "        out[f'ward_{t}_rate'] = out['ward'].map(ward_agg).fillna(train_fold[t].mean())\n",
    "        \n",
    "        # Topic category adoption rates\n",
    "        topic_agg = train_fold.groupby('topic_category')[t].mean().to_dict()\n",
    "        out[f'topiccat_{t}_rate'] = out['topic_category'].map(topic_agg).fillna(train_fold[t].mean())\n",
    "        \n",
    "        # Group adoption rates\n",
    "        group_agg = train_fold.groupby('group_name')[t].mean().to_dict()\n",
    "        out[f'group_{t}_rate'] = out['group_name'].map(group_agg).fillna(train_fold[t].mean())\n",
    "    \n",
    "    return out\n",
    "\n",
    "print(\"Feature engineering functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a6ee48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base features built in 45.9s\n",
      "Train: (16000, 43), Test: (6000, 40)\n",
      "26 new base columns: ['training_month', 'training_dow', 'training_quarter', 'training_day_of_year', 'training_week', 'days_since_ref', 'is_weekend', 'topic_category', 'farmer_prior_trainings', 'is_first_training', 'days_since_last_training', 'farmer_total_in_train', 'farmer_in_train', 'trainer_volume', 'group_volume', 'topic_volume', 'county_volume', 'subcounty_volume', 'ward_volume', 'topic_frequency', 'coop_x_has_topic', 'group_coop_rate', 'group_unique_farmers', 'trainer_unique_farmers', 'trainer_unique_topics', 'county_unique_farmers']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 4: Apply Base Features\n",
    "# ============================================================\n",
    "t0 = time.time()\n",
    "\n",
    "train_fe = build_base_features(train_df, train_ref=train_df)\n",
    "test_fe  = build_base_features(test_df,  train_ref=train_df)\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"Base features built in {elapsed:.1f}s\")\n",
    "print(f\"Train: {train_fe.shape}, Test: {test_fe.shape}\")\n",
    "\n",
    "new_cols = [c for c in train_fe.columns if c not in train_df.columns]\n",
    "print(f\"{len(new_cols)} new base columns: {new_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a48dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base features: 37\n",
      "Target-agg features (per fold): 24\n",
      "Total features per fold: 61\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Define Feature Columns & Label Encode\n",
    "# ============================================================\n",
    "CAT_COLS = [\n",
    "    'gender', 'registration', 'age', 'trainer',\n",
    "    'group_name', 'county', 'subcounty', 'ward',\n",
    "    'topics', 'topic_category',\n",
    "]\n",
    "\n",
    "# Base numeric features (no target info)\n",
    "BASE_NUM_COLS = [\n",
    "    'belong_to_cooperative', 'has_topic_trained_on',\n",
    "    'training_month', 'training_dow', 'training_quarter',\n",
    "    'training_day_of_year', 'training_week', 'days_since_ref', 'is_weekend',\n",
    "    'farmer_prior_trainings', 'is_first_training', 'days_since_last_training',\n",
    "    'farmer_total_in_train', 'farmer_in_train',\n",
    "    'trainer_volume', 'group_volume', 'topic_volume',\n",
    "    'county_volume', 'subcounty_volume', 'ward_volume',\n",
    "    'topic_frequency', 'coop_x_has_topic',\n",
    "    'group_coop_rate', 'group_unique_farmers',\n",
    "    'trainer_unique_farmers', 'trainer_unique_topics',\n",
    "    'county_unique_farmers',\n",
    "]\n",
    "\n",
    "# Target-dependent columns (added per-fold) — listed here for reference\n",
    "TARGET_AGG_COLS = []\n",
    "for t in TARGET_COLS:\n",
    "    TARGET_AGG_COLS.extend([\n",
    "        f'farmer_{t}_rate', f'farmer_{t}_sum', f'farmer_{t}_count',\n",
    "        f'trainer_{t}_rate', f'county_{t}_rate', f'ward_{t}_rate',\n",
    "        f'topiccat_{t}_rate', f'group_{t}_rate',\n",
    "    ])\n",
    "\n",
    "BASE_FEATURE_COLS = CAT_COLS + BASE_NUM_COLS  # used for base data\n",
    "ALL_FEATURE_COLS = BASE_FEATURE_COLS + TARGET_AGG_COLS  # full feature set per fold\n",
    "\n",
    "# Label encode categoricals\n",
    "label_encoders = {}\n",
    "for col in CAT_COLS:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train_fe[col], test_fe[col]], axis=0).astype(str)\n",
    "    le.fit(combined)\n",
    "    train_fe[col] = le.transform(train_fe[col].astype(str))\n",
    "    test_fe[col]  = le.transform(test_fe[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"Base features: {len(BASE_FEATURE_COLS)}\")\n",
    "print(f\"Target-agg features (per fold): {len(TARGET_AGG_COLS)}\")\n",
    "print(f\"Total features per fold: {len(ALL_FEATURE_COLS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "732eabc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoding & metric functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 6: Target Encoding + Competition Metric\n",
    "# ============================================================\n",
    "TE_COLS = ['ward', 'group_name', 'topics', 'trainer', 'subcounty', 'county']\n",
    "\n",
    "def add_target_encoding(train_x, train_y, test_x, cols, smooth=10):\n",
    "    \"\"\"Smoothed target encoding — applied per fold.\"\"\"\n",
    "    global_mean = train_y.mean()\n",
    "    new_train = train_x.copy()\n",
    "    new_test = test_x.copy()\n",
    "    for col in cols:\n",
    "        stats = train_x[[col]].copy()\n",
    "        stats['_target'] = train_y.values\n",
    "        agg = stats.groupby(col)['_target'].agg(['mean', 'count'])\n",
    "        agg['te'] = (agg['count'] * agg['mean'] + smooth * global_mean) / (agg['count'] + smooth)\n",
    "        te_map = agg['te'].to_dict()\n",
    "        new_train[f'{col}_te'] = new_train[col].map(te_map).fillna(global_mean)\n",
    "        new_test[f'{col}_te']  = new_test[col].map(te_map).fillna(global_mean)\n",
    "    return new_train, new_test\n",
    "\n",
    "def competition_metric(y_true, y_pred):\n",
    "    \"\"\"75% LogLoss + 25% (1 - ROC_AUC). Lower = better.\"\"\"\n",
    "    ll = log_loss(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    combined = 0.75 * ll + 0.25 * (1 - auc)\n",
    "    return ll, auc, combined\n",
    "\n",
    "print(\"Target encoding & metric functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f9531eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna tuning (20 trials)...\n",
      "\n",
      "Optuna done in 63s\n",
      "Best combined score: 0.18227\n",
      "Best params: {'learning_rate': 0.011615865989246453, 'num_leaves': 122, 'max_depth': 12, 'min_child_samples': 166, 'subsample': 0.6523068845866853, 'colsample_bytree': 0.5488360570031919, 'reg_alpha': 0.2637333993381525, 'reg_lambda': 0.015876781526923997}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Optuna Hyperparameter Tuning\n",
    "# ============================================================\n",
    "N_FOLDS_TUNE = 3\n",
    "TUNE_TARGET = 'adopted_within_07_days'\n",
    "\n",
    "# ID columns needed for target-agg feature mapping\n",
    "ID_MAP_COLS = ['farmer_id', 'trainer', 'county', 'ward', 'topic_category', 'group_name']\n",
    "\n",
    "def optuna_objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 15, 127),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 20, 200),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'subsample_freq': 1,\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 10.0, log=True),\n",
    "        'max_bin': 255,\n",
    "        'random_state': SEED,\n",
    "        'verbose': -1,\n",
    "    }\n",
    "    \n",
    "    y = train_fe[TUNE_TARGET].values\n",
    "    # Include ID columns alongside features for mapping\n",
    "    extra_cols = [c for c in ID_MAP_COLS if c not in BASE_FEATURE_COLS]\n",
    "    X_base = train_fe[BASE_FEATURE_COLS + extra_cols].copy()\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS_TUNE, shuffle=True, random_state=SEED)\n",
    "    oof = np.zeros(len(y))\n",
    "    \n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_base, y)):\n",
    "        X_tr = X_base.iloc[tr_idx].copy()\n",
    "        X_val = X_base.iloc[val_idx].copy()\n",
    "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "        \n",
    "        # Add target-agg features from training fold only\n",
    "        train_fold_df = train_fe.iloc[tr_idx]\n",
    "        X_tr = add_target_agg_features(X_tr, train_fold_df, TARGET_COLS)\n",
    "        X_val = add_target_agg_features(X_val, train_fold_df, TARGET_COLS)\n",
    "        \n",
    "        # Add target encoding\n",
    "        X_tr, X_val = add_target_encoding(\n",
    "            X_tr, pd.Series(y_tr, index=X_tr.index), X_val, TE_COLS\n",
    "        )\n",
    "        \n",
    "        # Drop ID mapping cols that aren't features\n",
    "        drop_cols = [c for c in extra_cols if c in X_tr.columns and c not in BASE_FEATURE_COLS]\n",
    "        X_tr = X_tr.drop(columns=drop_cols, errors='ignore')\n",
    "        X_val = X_val.drop(columns=drop_cols, errors='ignore')\n",
    "        \n",
    "        feat_cols = list(X_tr.columns)\n",
    "        dtrain = lgb.Dataset(X_tr, label=y_tr, categorical_feature=CAT_COLS, free_raw_data=False)\n",
    "        dval = lgb.Dataset(X_val, label=y_val, categorical_feature=CAT_COLS, reference=dtrain, free_raw_data=False)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params, dtrain, num_boost_round=2000, valid_sets=[dval],\n",
    "            callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        oof[val_idx] = np.clip(model.predict(X_val[feat_cols]), 1e-7, 1 - 1e-7)\n",
    "    \n",
    "    ll, auc, combined = competition_metric(y, oof)\n",
    "    return combined\n",
    "\n",
    "print(\"Starting Optuna tuning (20 trials)...\")\n",
    "t0 = time.time()\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "study.optimize(optuna_objective, n_trials=20, show_progress_bar=False)\n",
    "\n",
    "print(f\"\\nOptuna done in {time.time()-t0:.0f}s\")\n",
    "print(f\"Best combined score: {study.best_value:.5f}\")\n",
    "print(f\"Best params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20359d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM params: {'objective': 'binary', 'metric': 'binary_logloss', 'boosting_type': 'gbdt', 'max_bin': 255, 'random_state': 42, 'verbose': -1, 'subsample_freq': 1, 'learning_rate': 0.011615865989246453, 'num_leaves': 122, 'max_depth': 12, 'min_child_samples': 166, 'subsample': 0.6523068845866853, 'colsample_bytree': 0.5488360570031919, 'reg_alpha': 0.2637333993381525, 'reg_lambda': 0.015876781526923997}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Training LightGBM: adopted_within_07_days (pos_rate=0.1565)\n",
      "============================================================\n",
      "  Fold 1: LL=0.19282 AUC=0.96578 Combined=0.15317 (iter=316)\n",
      "  Fold 2: LL=0.19070 AUC=0.96806 Combined=0.15101 (iter=346)\n",
      "  Fold 3: LL=0.19452 AUC=0.96567 Combined=0.15447 (iter=473)\n",
      "  Fold 4: LL=0.19701 AUC=0.96422 Combined=0.15670 (iter=376)\n",
      "  Fold 5: LL=0.17643 AUC=0.96890 Combined=0.14009 (iter=457)\n",
      "  >>> OOF: LL=0.19029  AUC=0.96461  Combined=0.15157\n",
      "\n",
      "============================================================\n",
      "Training LightGBM: adopted_within_90_days (pos_rate=0.3469)\n",
      "============================================================\n",
      "  Fold 1: LL=0.20526 AUC=0.97655 Combined=0.15981 (iter=252)\n",
      "  Fold 2: LL=0.22876 AUC=0.96992 Combined=0.17909 (iter=242)\n",
      "  Fold 3: LL=0.22022 AUC=0.97319 Combined=0.17187 (iter=253)\n",
      "  Fold 4: LL=0.20852 AUC=0.97484 Combined=0.16268 (iter=249)\n",
      "  Fold 5: LL=0.23206 AUC=0.96877 Combined=0.18185 (iter=228)\n",
      "  >>> OOF: LL=0.21896  AUC=0.96602  Combined=0.17272\n",
      "\n",
      "============================================================\n",
      "Training LightGBM: adopted_within_120_days (pos_rate=0.4477)\n",
      "============================================================\n",
      "  Fold 1: LL=0.23431 AUC=0.98177 Combined=0.18029 (iter=226)\n",
      "  Fold 2: LL=0.20435 AUC=0.98523 Combined=0.15696 (iter=244)\n",
      "  Fold 3: LL=0.21114 AUC=0.98190 Combined=0.16288 (iter=242)\n",
      "  Fold 4: LL=0.20060 AUC=0.98535 Combined=0.15411 (iter=244)\n",
      "  Fold 5: LL=0.21742 AUC=0.98346 Combined=0.16720 (iter=244)\n",
      "  >>> OOF: LL=0.21356  AUC=0.98260  Combined=0.16452\n",
      "\n",
      "============================================================\n",
      "LightGBM training complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 8: LightGBM Training with Best Params + Proper CV\n",
    "# ============================================================\n",
    "N_FOLDS = 5\n",
    "NUM_BOOST_ROUND = 3000\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_bin': 255,\n",
    "    'random_state': SEED,\n",
    "    'verbose': -1,\n",
    "    'subsample_freq': 1,\n",
    "}\n",
    "lgb_params.update(study.best_params)\n",
    "print(f\"LightGBM params: {lgb_params}\\n\")\n",
    "\n",
    "extra_cols = [c for c in ID_MAP_COLS if c not in BASE_FEATURE_COLS]\n",
    "X_base_train = train_fe[BASE_FEATURE_COLS + extra_cols].copy()\n",
    "X_base_test  = test_fe[BASE_FEATURE_COLS + extra_cols].copy()\n",
    "\n",
    "oof_preds_lgb = {}\n",
    "test_preds_lgb = {}\n",
    "cv_scores = {}\n",
    "\n",
    "for target in TARGET_COLS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training LightGBM: {target} (pos_rate={train_fe[target].mean():.4f})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    y = train_fe[target].values\n",
    "    oof = np.zeros(len(train_fe))\n",
    "    test_pred_folds = np.zeros(len(test_fe))\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_base_train, y)):\n",
    "        X_tr = X_base_train.iloc[train_idx].copy()\n",
    "        X_val = X_base_train.iloc[val_idx].copy()\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        X_te = X_base_test.copy()\n",
    "        \n",
    "        # Add target-agg features from TRAINING FOLD only\n",
    "        train_fold_df = train_fe.iloc[train_idx]\n",
    "        X_tr = add_target_agg_features(X_tr, train_fold_df, TARGET_COLS)\n",
    "        X_val = add_target_agg_features(X_val, train_fold_df, TARGET_COLS)\n",
    "        X_te = add_target_agg_features(X_te, train_fold_df, TARGET_COLS)\n",
    "        \n",
    "        # Target encoding within fold\n",
    "        X_tr, X_val = add_target_encoding(\n",
    "            X_tr, pd.Series(y_tr, index=X_tr.index), X_val, TE_COLS\n",
    "        )\n",
    "        X_tr, X_te = add_target_encoding(\n",
    "            X_tr, pd.Series(y_tr, index=X_tr.index), X_te, TE_COLS\n",
    "        )\n",
    "        \n",
    "        # Drop ID mapping cols\n",
    "        drop_cols = [c for c in extra_cols if c in X_tr.columns and c not in BASE_FEATURE_COLS]\n",
    "        X_tr = X_tr.drop(columns=drop_cols, errors='ignore')\n",
    "        X_val = X_val.drop(columns=drop_cols, errors='ignore')\n",
    "        X_te = X_te.drop(columns=drop_cols, errors='ignore')\n",
    "        \n",
    "        feat_cols = list(X_tr.columns)\n",
    "        \n",
    "        dtrain = lgb.Dataset(X_tr[feat_cols], label=y_tr,\n",
    "                             categorical_feature=CAT_COLS, free_raw_data=False)\n",
    "        dval = lgb.Dataset(X_val[feat_cols], label=y_val,\n",
    "                           categorical_feature=CAT_COLS, reference=dtrain, free_raw_data=False)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            lgb_params, dtrain, num_boost_round=NUM_BOOST_ROUND,\n",
    "            valid_sets=[dval],\n",
    "            callbacks=[lgb.early_stopping(150, verbose=False), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        val_pred = np.clip(model.predict(X_val[feat_cols]), 1e-7, 1 - 1e-7)\n",
    "        test_pred = np.clip(model.predict(X_te[feat_cols]), 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        oof[val_idx] = val_pred\n",
    "        test_pred_folds += test_pred / N_FOLDS\n",
    "        \n",
    "        ll, auc, combined = competition_metric(y_val, val_pred)\n",
    "        print(f\"  Fold {fold+1}: LL={ll:.5f} AUC={auc:.5f} Combined={combined:.5f} (iter={model.best_iteration})\")\n",
    "    \n",
    "    oof = np.clip(oof, 1e-7, 1 - 1e-7)\n",
    "    oof_ll, oof_auc, oof_combined = competition_metric(y, oof)\n",
    "    print(f\"  >>> OOF: LL={oof_ll:.5f}  AUC={oof_auc:.5f}  Combined={oof_combined:.5f}\")\n",
    "    \n",
    "    oof_preds_lgb[target] = oof\n",
    "    test_preds_lgb[target] = test_pred_folds\n",
    "    cv_scores[target] = (oof_ll, oof_auc, oof_combined)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LightGBM training complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c49d5479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LR: adopted_within_07_days\n",
      "  OOF: LL=0.29925  AUC=0.96193\n",
      "Training LR: adopted_within_90_days\n",
      "  OOF: LL=0.22872  AUC=0.97358\n",
      "Training LR: adopted_within_120_days\n",
      "  OOF: LL=0.17266  AUC=0.97972\n",
      "\n",
      "Logistic Regression training complete.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 9: Logistic Regression Baseline (for ensemble)\n",
    "# ============================================================\n",
    "oof_preds_lr = {}\n",
    "test_preds_lr = {}\n",
    "\n",
    "# Include ID columns for mapping\n",
    "lr_cols = BASE_FEATURE_COLS + [c for c in ID_MAP_COLS if c not in BASE_FEATURE_COLS]\n",
    "train_lr = add_target_agg_features(train_fe[lr_cols].copy(), train_fe, TARGET_COLS)\n",
    "test_lr  = add_target_agg_features(test_fe[lr_cols].copy(), train_fe, TARGET_COLS)\n",
    "\n",
    "# Drop ID columns that aren't features, fill NaN\n",
    "drop_id = [c for c in ID_MAP_COLS if c not in BASE_FEATURE_COLS]\n",
    "train_lr = train_lr.drop(columns=drop_id, errors='ignore')\n",
    "test_lr  = test_lr.drop(columns=drop_id, errors='ignore')\n",
    "\n",
    "train_lr = train_lr.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "test_lr  = test_lr.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "lr_features = list(train_lr.columns)\n",
    "\n",
    "for target in TARGET_COLS:\n",
    "    print(f\"Training LR: {target}\")\n",
    "    y = train_fe[target].values\n",
    "    oof = np.zeros(len(train_fe))\n",
    "    test_pred_folds = np.zeros(len(test_fe))\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_lr, y)):\n",
    "        X_tr = train_lr.iloc[train_idx]\n",
    "        X_val = train_lr.iloc[val_idx]\n",
    "        y_tr = y[train_idx]\n",
    "        \n",
    "        lr = LogisticRegression(max_iter=3000, C=0.1, class_weight='balanced', random_state=SEED)\n",
    "        lr.fit(X_tr, y_tr)\n",
    "        \n",
    "        oof[val_idx] = np.clip(lr.predict_proba(X_val)[:, 1], 1e-7, 1 - 1e-7)\n",
    "        test_pred_folds += np.clip(lr.predict_proba(test_lr)[:, 1], 1e-7, 1 - 1e-7) / N_FOLDS\n",
    "    \n",
    "    ll, auc, _ = competition_metric(y, oof)\n",
    "    print(f\"  OOF: LL={ll:.5f}  AUC={auc:.5f}\")\n",
    "    oof_preds_lr[target] = oof\n",
    "    test_preds_lr[target] = test_pred_folds\n",
    "\n",
    "print(\"\\nLogistic Regression training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "260e556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adopted_within_07_days: best_w_lgb=0.75  LL=0.15466  AUC=0.97489  Combined=0.12227\n",
      "adopted_within_90_days: best_w_lgb=0.50  LL=0.15916  AUC=0.98538  Combined=0.12302\n",
      "adopted_within_120_days: best_w_lgb=0.50  LL=0.14678  AUC=0.98897  Combined=0.11284\n",
      "\n",
      "Ensemble weights found.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 10: Find Optimal Ensemble Weights via OOF\n",
    "# ============================================================\n",
    "# Search for best blend weight that minimizes combined metric on OOF\n",
    "\n",
    "best_weights = {}\n",
    "oof_preds_blend = {}\n",
    "test_preds_blend = {}\n",
    "\n",
    "for target in TARGET_COLS:\n",
    "    y = train_fe[target].values\n",
    "    best_score = 999\n",
    "    best_w = 1.0  # default: all LightGBM\n",
    "    \n",
    "    for w_lgb in np.arange(0.5, 1.01, 0.05):\n",
    "        blend = w_lgb * oof_preds_lgb[target] + (1 - w_lgb) * oof_preds_lr[target]\n",
    "        blend = np.clip(blend, 1e-7, 1 - 1e-7)\n",
    "        _, _, combined = competition_metric(y, blend)\n",
    "        if combined < best_score:\n",
    "            best_score = combined\n",
    "            best_w = w_lgb\n",
    "    \n",
    "    best_weights[target] = best_w\n",
    "    \n",
    "    # Apply best weight\n",
    "    oof_blend = best_w * oof_preds_lgb[target] + (1 - best_w) * oof_preds_lr[target]\n",
    "    test_blend = best_w * test_preds_lgb[target] + (1 - best_w) * test_preds_lr[target]\n",
    "    \n",
    "    oof_preds_blend[target] = np.clip(oof_blend, 1e-7, 1 - 1e-7)\n",
    "    test_preds_blend[target] = np.clip(test_blend, 1e-7, 1 - 1e-7)\n",
    "    \n",
    "    ll, auc, combined = competition_metric(y, oof_preds_blend[target])\n",
    "    print(f\"{target}: best_w_lgb={best_w:.2f}  LL={ll:.5f}  AUC={auc:.5f}  Combined={combined:.5f}\")\n",
    "\n",
    "print(\"\\nEnsemble weights found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e63bb551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL CROSS-VALIDATION SUMMARY (LGB + LR Ensemble)\n",
      "======================================================================\n",
      "  adopted_within_07_days        : LL=0.15466  AUC=0.97489  Comb=0.12227  w_lgb=0.75\n",
      "  adopted_within_90_days        : LL=0.15916  AUC=0.98538  Comb=0.12302  w_lgb=0.50\n",
      "  adopted_within_120_days       : LL=0.14678  AUC=0.98897  Comb=0.11284  w_lgb=0.50\n",
      "\n",
      "  AVERAGE                       : LL=0.15353  AUC=0.98308  Comb=0.11938\n",
      "  LGB-ONLY                      : LL=0.20761  AUC=0.97108  Comb=0.16294\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 11: CV Summary\n",
    "# ============================================================\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL CROSS-VALIDATION SUMMARY (LGB + LR Ensemble)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_ll = total_auc = 0\n",
    "for target in TARGET_COLS:\n",
    "    y = train_fe[target].values\n",
    "    ll, auc, combined = competition_metric(y, oof_preds_blend[target])\n",
    "    total_ll += ll\n",
    "    total_auc += auc\n",
    "    w = best_weights[target]\n",
    "    print(f\"  {target:30s}: LL={ll:.5f}  AUC={auc:.5f}  Comb={combined:.5f}  w_lgb={w:.2f}\")\n",
    "\n",
    "avg_ll = total_ll / 3\n",
    "avg_auc = total_auc / 3\n",
    "avg_comb = 0.75 * avg_ll + 0.25 * (1 - avg_auc)\n",
    "print(f\"\\n  {'AVERAGE':30s}: LL={avg_ll:.5f}  AUC={avg_auc:.5f}  Comb={avg_comb:.5f}\")\n",
    "\n",
    "# Compare to LGB-only\n",
    "total_ll_lgb = total_auc_lgb = 0\n",
    "for target in TARGET_COLS:\n",
    "    y = train_fe[target].values\n",
    "    ll, auc, _ = competition_metric(y, oof_preds_lgb[target])\n",
    "    total_ll_lgb += ll; total_auc_lgb += auc\n",
    "avg_ll_lgb = total_ll_lgb / 3\n",
    "avg_auc_lgb = total_auc_lgb / 3\n",
    "avg_comb_lgb = 0.75 * avg_ll_lgb + 0.25 * (1 - avg_auc_lgb)\n",
    "print(f\"  {'LGB-ONLY':30s}: LL={avg_ll_lgb:.5f}  AUC={avg_auc_lgb:.5f}  Comb={avg_comb_lgb:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b26fbfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monotonicity violations: 1193 -> 0\n",
      "\n",
      "Prediction means (should increase):\n",
      "  07-day:  0.0933\n",
      "  90-day:  0.1623\n",
      "  120-day: 0.1874\n",
      "\n",
      "All checks passed! Saved: submission_v3_ensemble.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target_07_AUC</th>\n",
       "      <th>Target_07_LogLoss</th>\n",
       "      <th>Target_90_AUC</th>\n",
       "      <th>Target_90_LogLoss</th>\n",
       "      <th>Target_120_AUC</th>\n",
       "      <th>Target_120_LogLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_6AA1EM</td>\n",
       "      <td>0.144599</td>\n",
       "      <td>0.144599</td>\n",
       "      <td>0.178587</td>\n",
       "      <td>0.178587</td>\n",
       "      <td>0.178587</td>\n",
       "      <td>0.178587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_2DV3A1</td>\n",
       "      <td>0.011193</td>\n",
       "      <td>0.011193</td>\n",
       "      <td>0.021591</td>\n",
       "      <td>0.021591</td>\n",
       "      <td>0.034780</td>\n",
       "      <td>0.034780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_KZY5B8</td>\n",
       "      <td>0.525251</td>\n",
       "      <td>0.525251</td>\n",
       "      <td>0.864765</td>\n",
       "      <td>0.864765</td>\n",
       "      <td>0.952133</td>\n",
       "      <td>0.952133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_T8WZT2</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.010824</td>\n",
       "      <td>0.010824</td>\n",
       "      <td>0.014418</td>\n",
       "      <td>0.014418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_3CX56O</td>\n",
       "      <td>0.061188</td>\n",
       "      <td>0.061188</td>\n",
       "      <td>0.138201</td>\n",
       "      <td>0.138201</td>\n",
       "      <td>0.271528</td>\n",
       "      <td>0.271528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID_ARP4MA</td>\n",
       "      <td>0.020877</td>\n",
       "      <td>0.020877</td>\n",
       "      <td>0.020877</td>\n",
       "      <td>0.020877</td>\n",
       "      <td>0.032458</td>\n",
       "      <td>0.032458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID_NT05CR</td>\n",
       "      <td>0.086030</td>\n",
       "      <td>0.086030</td>\n",
       "      <td>0.086030</td>\n",
       "      <td>0.086030</td>\n",
       "      <td>0.086030</td>\n",
       "      <td>0.086030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ID_404ITE</td>\n",
       "      <td>0.948769</td>\n",
       "      <td>0.948769</td>\n",
       "      <td>0.967951</td>\n",
       "      <td>0.967951</td>\n",
       "      <td>0.969739</td>\n",
       "      <td>0.969739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ID_19GWB0</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.013450</td>\n",
       "      <td>0.013450</td>\n",
       "      <td>0.019882</td>\n",
       "      <td>0.019882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ID_OZDQ0S</td>\n",
       "      <td>0.567498</td>\n",
       "      <td>0.567498</td>\n",
       "      <td>0.833248</td>\n",
       "      <td>0.833248</td>\n",
       "      <td>0.833248</td>\n",
       "      <td>0.833248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  Target_07_AUC  Target_07_LogLoss  Target_90_AUC  \\\n",
       "0  ID_6AA1EM       0.144599           0.144599       0.178587   \n",
       "1  ID_2DV3A1       0.011193           0.011193       0.021591   \n",
       "2  ID_KZY5B8       0.525251           0.525251       0.864765   \n",
       "3  ID_T8WZT2       0.003935           0.003935       0.010824   \n",
       "4  ID_3CX56O       0.061188           0.061188       0.138201   \n",
       "5  ID_ARP4MA       0.020877           0.020877       0.020877   \n",
       "6  ID_NT05CR       0.086030           0.086030       0.086030   \n",
       "7  ID_404ITE       0.948769           0.948769       0.967951   \n",
       "8  ID_19GWB0       0.010000           0.010000       0.013450   \n",
       "9  ID_OZDQ0S       0.567498           0.567498       0.833248   \n",
       "\n",
       "   Target_90_LogLoss  Target_120_AUC  Target_120_LogLoss  \n",
       "0           0.178587        0.178587            0.178587  \n",
       "1           0.021591        0.034780            0.034780  \n",
       "2           0.864765        0.952133            0.952133  \n",
       "3           0.010824        0.014418            0.014418  \n",
       "4           0.138201        0.271528            0.271528  \n",
       "5           0.020877        0.032458            0.032458  \n",
       "6           0.086030        0.086030            0.086030  \n",
       "7           0.967951        0.969739            0.969739  \n",
       "8           0.013450        0.019882            0.019882  \n",
       "9           0.833248        0.833248            0.833248  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 12: Generate Submission with Monotonicity\n",
    "# ============================================================\n",
    "ss = pd.read_csv('SampleSubmission.csv')\n",
    "\n",
    "p07  = np.clip(test_preds_blend['adopted_within_07_days'],  1e-7, 1 - 1e-7)\n",
    "p90  = np.clip(test_preds_blend['adopted_within_90_days'],  1e-7, 1 - 1e-7)\n",
    "p120 = np.clip(test_preds_blend['adopted_within_120_days'], 1e-7, 1 - 1e-7)\n",
    "\n",
    "# ENFORCE MONOTONICITY: P(7d) <= P(90d) <= P(120d)\n",
    "p90_fixed  = np.maximum(p90, p07)     # 90d must be >= 7d\n",
    "p120_fixed = np.maximum(p120, p90_fixed)  # 120d must be >= 90d\n",
    "p07_fixed  = np.minimum(p07, p90_fixed)   # safety\n",
    "\n",
    "violations_before = (p07 > p90).sum() + (p90 > p120).sum()\n",
    "violations_after = (p07_fixed > p90_fixed).sum() + (p90_fixed > p120_fixed).sum()\n",
    "print(f\"Monotonicity violations: {violations_before} -> {violations_after}\")\n",
    "\n",
    "ss['Target_07_AUC']     = p07_fixed\n",
    "ss['Target_07_LogLoss']  = p07_fixed\n",
    "ss['Target_90_AUC']     = p90_fixed\n",
    "ss['Target_90_LogLoss']  = p90_fixed\n",
    "ss['Target_120_AUC']    = p120_fixed\n",
    "ss['Target_120_LogLoss'] = p120_fixed\n",
    "\n",
    "# Sanity checks\n",
    "expected = ['ID', 'Target_07_AUC', 'Target_07_LogLoss', \n",
    "            'Target_90_AUC', 'Target_90_LogLoss',\n",
    "            'Target_120_AUC', 'Target_120_LogLoss']\n",
    "assert list(ss.columns) == expected\n",
    "assert len(ss) == len(test_df)\n",
    "for col in expected[1:]:\n",
    "    assert ss[col].min() > 0 and ss[col].max() < 1 and not ss[col].isna().any()\n",
    "assert (ss['Target_07_AUC'] <= ss['Target_90_AUC'] + 1e-9).all()\n",
    "assert (ss['Target_90_AUC'] <= ss['Target_120_AUC'] + 1e-9).all()\n",
    "\n",
    "print(f\"\\nPrediction means (should increase):\")\n",
    "print(f\"  07-day:  {ss['Target_07_AUC'].mean():.4f}\")\n",
    "print(f\"  90-day:  {ss['Target_90_AUC'].mean():.4f}\")\n",
    "print(f\"  120-day: {ss['Target_120_AUC'].mean():.4f}\")\n",
    "\n",
    "ss.to_csv('submission_v3_ensemble.csv', index=False)\n",
    "print(f\"\\nAll checks passed! Saved: submission_v3_ensemble.csv\")\n",
    "ss.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
