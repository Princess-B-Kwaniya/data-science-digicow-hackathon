DIGICOW Hackathon - Solution Explanation
=========================================

COMPETITION OVERVIEW
--------------------
This is a Zindi data science competition for predicting dairy technology adoption 
by farmers in Kenya. The DigiCow platform trains farmers and we predict whether 
they adopt the technology within 7, 90, and 120 days of their first training.

The evaluation metric is a composite score across 3 targets, each scored as:
  Score = 0.75 * LogLoss + 0.25 * (1 - AUC)
Final score = average of the 3 target scores (LOWER is better).

Each submission has 6 prediction columns:
  - Target_07_AUC, Target_07_LogLoss
  - Target_90_AUC, Target_90_LogLoss
  - Target_120_AUC, Target_120_LogLoss


DATA
----
- Train.csv: ~14,000 farmer training records with 3 binary targets
- Test.csv:  ~5,000 farmer records to predict
- Key features: county, ward, subcounty, trainer, farmer_id, group_name, 
  has_topic_trained_on, belong_to_cooperative, training_date

The data is tabular with mostly categorical features. There are geographic 
hierarchies (county > subcounty > ward) and organizational groups.


APPROACH THAT ACHIEVED HIGHEST SCORE
--------------------------------------
Best Leaderboard Score: 0.7247 (submission: sub_V9_A_v7_topicfix.csv)

The solution is built in three stages:

### Stage 1: GROUPS_V3_FAST.py -> sub_V3_A_optimal.csv (LB: 0.722)

This uses Bayesian hierarchical group-based predictions (NO machine learning model).

Core idea: For each target, predictions are made using group-level adoption rates 
with Bayesian smoothing toward the global mean.

  Bayesian prediction = (group_count * group_mean + s * global_mean) / (group_count + s)

Where s is the smoothing parameter (higher s = more conservative, closer to global mean).

The hierarchy works by cascading through group keys from most specific to most general:
  1. trainer_ward_topic (most specific: which trainer, which ward, which topic)
  2. trainer_ward
  3. ward_topic
  4. county_topic
  5. trainer
  6. county
  7. ward (most general)

For each test row:
  - Try the most specific group first
  - If the group exists in training data, use its Bayesian-smoothed prediction
  - If not, fall through to the next less-specific group
  - Final fallback is the global adoption rate

The optimal smoothing parameter and hierarchy ordering were selected via 
5-fold stratified cross-validation, independently for each target.

Key insight: This approach works because the data has strong geographic and 
trainer-based patterns. Farmers in the same ward trained by the same trainer 
on the same topic have very similar adoption patterns.


### Stage 2: V7_SAFE.py -> sub_V7_D1_DUAL_fine_v3ll.csv (LB: 0.723)

This builds on V3 with a DUAL column strategy:

Since Zindi evaluates AUC and LogLoss from SEPARATE columns, we can optimize 
each independently:

  - LogLoss columns: Keep the V3_A_optimal predictions (well-calibrated, proven 0.722)
  - AUC columns: Use a FINER-grained hierarchy that produces more unique values

The fine-grained hierarchy for AUC includes ward_group and group_name:
  ward_group -> group_name -> ward_topic -> county_topic -> ward -> county -> trainer

More unique prediction values help AUC because AUC measures ranking ability.
V3 only had ~39 unique values; the fine hierarchy creates hundreds, giving 
the AUC metric more granularity to distinguish between farmers.

The LogLoss columns stay identical to V3 because LogLoss penalizes 
miscalibrated probabilities heavily - any aggressive changes hurt it 
(lesson learned from earlier farmer_id experiments that dropped to 0.702).


### Stage 3: V9_TOPICFIX.py -> sub_V9_A_v7_topicfix.csv (LB: 0.7247)

This is a targeted post-processing fix on top of V7:

Key discovery: The feature has_topic_trained_on is a binary (0/1) flag.
In training data, ALL 838 rows with has_topic_trained_on=0 have ZERO adoption
across all 3 targets. This means farmers who were not trained on a specific topic
never adopt the technology.

In the test set, 99 rows have has_topic_trained_on=0. V7 was predicting these
at 3-10% probability (from geographic group averages), but the true probability
is essentially 0%.

The fix: Force all predictions for has_topic_trained_on=0 test rows to 0.001
(minimum allowed value) in BOTH AUC and LogLoss columns.

Why this helps:
  - LogLoss: Predicting 0.001 instead of 0.03-0.10 for actual 0 labels gives
    much lower log loss. The penalty goes from -log(1-0.05)≈0.051 to -log(0.999)≈0.001.
  - AUC: Pushing guaranteed non-adopters to the minimum helps the ranking — they
    should be ranked below all other farmers.

This changed only 99 of 6000 test rows. No model retraining needed.


LEADERBOARD PROGRESSION
-----------------------
  V3_A_optimal (Stage 1):       0.722
  V7_D1 DUAL (Stage 2):         0.723  
  V9_A topic fix (Stage 3):     0.7247  (current best)


KEY LESSONS LEARNED
--------------------
1. Group-based Bayesian predictions beat ML models (LightGBM, CatBoost) on this 
   data because the categorical features have strong hierarchical structure.

2. Bayesian smoothing is critical - raw group means overfit badly, especially 
   for small groups. The smoothing parameter s controls the bias-variance tradeoff.

3. The DUAL column strategy (different predictions for AUC vs LogLoss) provides 
   a small but real improvement because the two metrics reward different properties.

4. Aggressive personalization (e.g., farmer_id with low smoothing) destroys 
   LogLoss even when it helps AUC - LogLoss has 3x the weight.

5. Cross-validation on this dataset is unreliable for small improvements 
   (<0.005) due to distribution shift between train and test.

6. Fine-grained hierarchies (ward_group, group_name) overfit in CV but hurt 
   test LogLoss badly — V8 changed the LL column and dropped to 0.695.

7. Domain knowledge beats brute-force optimization: the has_topic_trained_on=0 
   insight (guaranteed non-adoption) provided a safe, reliable improvement.


FILES IN THIS REPOSITORY
--------------------------
Solution code:
  - GROUPS_V3_FAST.py    : Stage 1 - generates the V3 base predictions (LB 0.722)
  - V7_SAFE.py           : Stage 2 - DUAL column improvement (LB 0.723)
  - V9_TOPICFIX.py       : Stage 3 - topic=0 post-processing fix (LB 0.7247, best)

Best submissions:
  - sub_V3_A_optimal.csv         : V3 base submission (LB 0.722)
  - sub_V7_D1_DUAL_fine_v3ll.csv : V7 DUAL submission (LB 0.723)
  - sub_V9_A_v7_topicfix.csv     : Best submission (LB 0.7247)

Data:
  - Train.csv                    : Training data (16,000 rows)
  - Test.csv                     : Test data (6,000 rows)
  - dataset_data_dictionary.csv  : Column descriptions
  - SampleSubmission.csv         : Submission format template

Notebooks:
  - StarterNotebook.ipynb        : Competition starter notebook
  - CompetitiveSolution.ipynb    : Competition-provided competitive solution
