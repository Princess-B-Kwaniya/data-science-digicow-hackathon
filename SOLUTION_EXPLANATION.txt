DIGICOW Hackathon - Solution Explanation
=========================================
Updated: February 14, 2026

COMPETITION OVERVIEW
--------------------
Zindi DigiCow competition — predicting dairy technology adoption by farmers in
Kenya. DigiCow trains farmers and we predict whether they adopt the technology
within 7, 90, and 120 days of training.

Evaluation metric (per target):
  Score = 0.75 * (1 - LogLoss) + 0.25 * AUC
  Final score = sum across 3 targets. HIGHER = BETTER.
  Maximum possible score = 3.0

Each submission has 6 prediction columns:
  - Target_07_AUC, Target_07_LogLoss
  - Target_90_AUC, Target_90_LogLoss  
  - Target_120_AUC, Target_120_LogLoss

⚠️  The dataset was completely refreshed mid-competition. All Phase 1 (old data)
scores are invalidated. This document covers Phase 2 (new data).

DATA (Phase 2 — New Dataset)
-----------------------------
- Train.csv:  13,536 farmer training records with 3 binary targets
- Test.csv:   5,621 farmer records to predict
- Prior.csv:  44,882 rows — HISTORICAL farmer training sessions (KEY RESOURCE)
- Target rates: 7d=1.13%, 90d=1.58%, 120d=2.23% (extreme imbalance)

Schema changes from old data:
  - farmer_name (was farmer_id)
  - training_day (was training_date)
  - topics_list: nested list-of-lists in Train/Test, flat list in Prior
  - trainer: list format in Train/Test, plain string in Prior

Key data properties:
  - ZERO farmer overlap between Train and Test
  - 62.7% of test farmers appear in Prior.csv (3,526 of 5,621)
  - Average 8.1 prior sessions per test farmer with history
  - Zero trainer overlap between Prior and Train/Test
  - Group overlap: Prior∩Test=210/239, Train∩Test=126 groups


BEST APPROACH — CompetitiveSolution.py (LB: 0.91788)
------------------------------------------------------
(This was our best SINGLE-MODEL approach. See "BLENDING BREAKTHROUGH" below
for how we reached 0.950636.)

Best Single-Model Score: 0.91788 (submission: sub_COMP_D_dual_prior_blend.csv)

### Core Pipeline

1. PRIOR AS FEATURE SOURCE (not training data!)
   Prior.csv provides 46 farmer/group/geo history features + 12 prior target
   encodings. These capture each farmer's historical adoption behavior.
   
   Farmer history (19 features): session counts, adoption rates per target,
   cooperative rate, unique groups/wards/trainers, topic diversity, training
   span, ever-adopted flags, adoption score.
   
   Group history (9 features): group size, adoption rates, cooperative/topic
   rates, unique farmers/trainers, adoption score.
   
   Geo history (18 features): ward/subcounty/county-level adoption rates,
   sizes, cooperative rates, has_topic rates.

2. LIGHTGBM ON TRAIN ONLY (13,536 rows)
   Single LightGBM model with 205 features, 5-fold StratifiedKFold CV.
   Params: num_leaves=63, lr=0.03, feature_fraction=0.75, bagging=0.8,
   scale_pos_weight per target (~44-88x for class imbalance).
   
   CV scores: 7d AUC=0.975/LL=0.033, 90d AUC=0.977/LL=0.035,
   120d AUC=0.981/LL=0.039. Estimated total: 2.903.

3. DUAL COLUMN STRATEGY
   AUC columns: rank-based (percentile * 0.998 + 0.001) — optimized for ranking
   LL columns: raw calibrated model predictions — optimized for calibration

4. PRIOR RATE BLENDING (70% model, 30% prior)
   For test farmers with prior history, blend model predictions with their
   known historical adoption rate. Anchors predictions to observed behavior.

5. POST-PROCESSING
   - has_topic_trained_on=0 → force 0.001 (24 test rows)
   - Groups with 30+ train samples and 0% adoption → cap at 0.005
   - Monotonicity enforcement: pred_7d ≤ pred_90d ≤ pred_120d
   - Clip all predictions to [0.001, 0.999]

### Feature Breakdown (205 total)
  - Prior farmer history:     19 features
  - Prior group history:       9 features
  - Prior geo history:        18 features
  - Temporal:                 16 features (year, month, day, cyclic, season, etc.)
  - Topic categories:         15 features (topic_count, 11 binary categories, etc.)
  - Geographic interactions:   8 features (county_subcounty, ward_trainer, etc.)
  - Frequency encoding:       11 features
  - Group features:            9 features (size, coop rate, diversity, etc.)
  - Trainer features:          5 features
  - Demographics:              9 interaction features
  - History interactions:      7 features
  - Target encoding OOF:      45 features (15 cols × 3 targets, smoothing=10)
  - Prior target encoding:    12 features (4 cols × 3 targets, smoothing=20)
  - Aggregations:              9 features
  - Other:                    13 features


LEADERBOARD PROGRESSION (Phase 2)
-----------------------------------

=== STAGE 1: Single Models (Feb 10-11) ===
  COMP_A  (standard LGB):               0.91748
  COMP_B  (DUAL):                        ~0.9175
  COMP_D  (DUAL + Prior blend):          0.91788
  ENS_G   (3-model, Prior+Train):        0.90705  X Prior training HURT
  REFv3_A (3-model, extra features):     0.88230  X Target leakage
  REFv3_D (60% D + 40% REFv3_A):        0.91254  X Bad blend poisoned D

=== STAGE 2: V4/V5 Multi-Seed Ensembles (Feb 11-12) ===
  ULTIMATE_V4 (LGB 20 seeds):           0.93892
  ULTIMATE_V5 (LGB+XGB+Stack):          0.93892
  Per-target progressive blend:          0.942924  (sub_ADV_pertarget_progressive.csv)

=== STAGE 3: Teammate Blending (Feb 12) ===
  Teammate's file: submission_v4_ensemble (3).csv
  (LGB+CatBoost+XGB ensemble with topic rates, farmer history, trainer-county combos)

  LL-only arithmetic blend 10% V4:       0.944093
  LL-only arithmetic blend 20% V4:       0.945115
  LL-only arithmetic blend 30% V4:       0.946010

=== STAGE 4: Rank Averaging Discovery (Feb 13) ===
  Rank-avg 30% V4:                       0.947179  (surprise winner!)
  Rank-avg 45% V4:                       0.948382
  Rank-avg 50% V4:                       0.948672
  Rank-avg 55% V4:                       0.948948
  Rank-avg 60% V4:                       0.949140
  Rank-avg 70% V4:                       0.949576
  Rank-avg 80% V4:                       0.949633
  Rank-avg 85% V4:                       0.949640  (LL rank-avg plateau)

=== STAGE 5: RANKLOGODDS + AUC Blending (Feb 14) ===
  RANKLOGODDS r80 d50 (LL only):         0.949815  (+175 over plain rank-avg!)
  RKLO r80 d60 (LL only):               0.949819
  RKLO r80 d60 + AUC 5%:                0.950109  (AUC blending breakthrough!)
  RKLO r80 d60 + AUC 10%:               0.950389
  RKLO r80 d60 + AUC 15%:               0.950636  *** CURRENT BEST ***


===============================================================================
BLENDING BREAKTHROUGH — DETAILED EXPLANATION
===============================================================================

OVERVIEW
---------
Our biggest gains came NOT from better models, but from a sophisticated
post-prediction blending pipeline that combines two independently-trained
model outputs. This took us from 0.942924 to 0.950636 (+0.007712).

The final best submission uses THREE key innovations stacked together:
  1. Rank-Averaged LogLoss columns (ordering from consensus)
  2. LogOdds distribution mapping (calibration from both sources)
  3. AUC column blending (attacking the remaining 25% of the metric)


STEP 1: THE DUAL COLUMN STRATEGY (foundation)
-----------------------------------------------
The competition metric is: Score = 0.75*(1-LogLoss) + 0.25*AUC per target.

Key insight: AUC only cares about RANKING, LogLoss cares about CALIBRATION.
So we use DIFFERENT values for each:
  - AUC columns: percentile ranks (rank/N * 0.998 + 0.001)
    -> Optimal for AUC since it preserves perfect ordering
  - LL columns: raw calibrated model probabilities
    -> Optimal for LogLoss since calibration matters

This means our "best" file (sub_ADV_pertarget_progressive.csv) has DIFFERENT
values in AUC vs LL columns — they are independently optimized.


STEP 2: LL-ONLY BLENDING (first breakthrough)
------------------------------------------------
When blending with teammate's file, we ONLY blend the LogLoss columns (75% of
metric weight) while keeping AUC columns from our best file UNTOUCHED.

Why? Our AUC columns are already rank-optimized (percentile-based). Blending
would destroy the rank optimization. But LL columns benefit from diversity —
two models' errors partially cancel out.

  Formula: blended_LL = (1-w) * best_LL + w * teammate_LL

This alone took us from 0.942924 to 0.946010 at 30% teammate weight.


STEP 3: RANK AVERAGING (second breakthrough, +0.003 over arithmetic)
----------------------------------------------------------------------
Instead of arithmetic mean of probabilities, we use RANK AVERAGING:

  1. Compute percentile ranks for both sources per LL column:
       r_best = rankdata(best[col]) / N
       r_v4   = rankdata(v4e3[col]) / N

  2. Weighted average of RANKS (not values):
       blended_ranks = (1-w) * r_best + w * r_v4

  3. Map blended ranks back to original value distribution:
       sorted_vals = sort(best[col])
       output[col] = sorted_vals[ordinal_rank(blended_ranks) - 1]

Why this works better than arithmetic blending:
  - Arithmetic mean pulls predictions toward 0.5 (hurts LogLoss for rare events)
  - Rank averaging preserves the DISTRIBUTION SHAPE of the original predictions
  - It uses consensus ORDERING from both models but keeps calibration intact
  - For 1-2% adoption rates, this is critical — we need very low predictions

At same V4 weight (30%): arithmetic=0.946010, rank-avg=0.947179 (+0.001169)
The rank-avg advantage grew with higher V4 weights, peaking at 85% V4.


STEP 4: RANKLOGODDS DISTRIBUTION (third breakthrough, +0.000175)
------------------------------------------------------------------
Plain rank-averaging maps back to ONE source's sorted distribution. But both
sources have useful calibration information. RANKLOGODDS blends the
distributions in logit (log-odds) space:

  1. Sort both sources' values:
       sorted_best = sort(best[col])
       sorted_v4   = sort(v4e3[col])

  2. Convert to logits and blend:
       b_logit = logit(sorted_best)
       v_logit = logit(sorted_v4)
       blended_dist = expit((1-d) * b_logit + d * v_logit)

  3. Map rank-averaged ordering to blended distribution:
       output[col] = blended_dist[ordinal_rank(blended_ranks) - 1]

Why logit space? For rare events (1-2%), logit is the natural scale for
calibration. A probability of 0.01 becomes logit=-4.6, while 0.02 becomes
logit=-3.9. Linear interpolation in logit space is more principled than
in probability space for extreme class imbalance.

Best config: rank_weight=80% V4, dist_ratio=60% V4.
This means: "Use 80% V4's ORDERING, with calibration slightly favoring V4's
distribution (60/40 blend in logit space)."


STEP 5: AUC COLUMN BLENDING (fourth breakthrough, +0.000817)
--------------------------------------------------------------
We had been keeping AUC columns 100% from our best file (rank-optimized).
But V4 has INDEPENDENT discriminative information. Even a small contribution
from V4's ranking can improve the AUC component (25% of metric).

Method: Apply same rank-averaging to AUC columns with SMALL V4 weight:
  r_best = rankdata(best[auc_col]) / N
  r_v4   = rankdata(v4e3[auc_col]) / N
  blended = (1-w) * r_best + w * r_v4
  output[auc_col] = sorted_best_auc[ordinal_rank(blended) - 1]

The AUC gains were nearly LINEAR:
  AUC 0%:  0.949819 (baseline)
  AUC 5%:  0.950109 (+290)
  AUC 10%: 0.950389 (+280)
  AUC 15%: 0.950636 (+246)  <-- CURRENT BEST

This dimension has NOT peaked yet. AUC 20-25% should yield further gains.


STEP 6: MONOTONICITY ENFORCEMENT (post-processing)
-----------------------------------------------------
After all blending, enforce that predictions are monotonically increasing
across time windows (a farmer who adopts within 7 days MUST have adopted
within 90 and 120 days):

  pred_90d = max(pred_90d, pred_7d)
  pred_120d = max(pred_120d, pred_90d)


FINAL FORMULA SUMMARY
-----------------------
Best submission: sub_BEST2_rklo80d60_auc15.csv (LB: 0.950636)

For each LogLoss column:
  1. ranks = 0.20 * rank(best) + 0.80 * rank(v4e3)      [80% V4 ordering]
  2. dist  = expit(0.40 * logit(sort(best)) + 0.60 * logit(sort(v4e3)))
  3. LL    = dist[ordinal_rank(ranks) - 1]                [logodds-blended calibration]

For each AUC column:
  1. ranks = 0.85 * rank(best) + 0.15 * rank(v4e3)       [15% V4 ordering]
  2. AUC   = sort(best)[ordinal_rank(ranks) - 1]          [best's distribution]

Then: enforce monotonicity on LL columns.

Source files:
  - sub_ADV_pertarget_progressive.csv  ("best" — our V4 multi-seed LGB)
  - submission_v4_ensemble (3).csv     ("v4e3" — teammate's LGB+CB+XGB ensemble)

Generation scripts:
  - gen_best.py   : First RKLO + AUC blending experiments
  - gen_best2.py  : Full AUC gradient grid (the winning approach)


WHAT'S NEXT
-------------
1. Continue AUC gradient: test 20%, 25%, 30% V4 AUC weight (still climbing)
2. RKLO on AUC too: instead of plain rank-avg for AUC, use logodds distribution
   mapping (sub_BEST2_rkloFULL_auc*.csv files ready)
3. Per-target AUC optimization: different AUC% per target
   (sub_BEST2_aucPT_*.csv files ready)
4. Generate a 3rd independent model source for triple-blending
5. Fine-grid around optimal LL parameters (rank=78-82%, dist=55-65%)


===============================================================================


KEY LESSONS LEARNED
---------------------
1. Prior.csv as FEATURES is the biggest signal source. 62.7% test coverage
   with farmer-level history enables powerful predictions.

2. NEVER train on Prior.csv data. Prior has higher adoption rates (1.5-4.7%
   vs Train's 1.1-2.2%). Mixing shifts predictions upward, destroying
   LogLoss calibration. ENS_G confirmed: 0.907 vs D's 0.918.

3. Adding features can HURT badly. REFINED_v3 added train_grp_*_rate
   (target leakage!) and other features. CV improved (2.945 vs 2.903)
   but LB dropped to 0.882. NEVER trust CV improvements from leaked features.

4. The DUAL column strategy provides consistent small gains (~0.0004).
   Rank-based AUC + calibrated LL is strictly better than same-column.

5. Prior rate blending (70/30) helps for farmers with known history.
   Anchoring to observed behavior reduces prediction error.

6. Don't blend with worse predictions. Even 60% proven D + 40% bad
   ensemble dropped D from 0.918 to 0.913.

7. The competition rewards low, well-calibrated predictions more than
   high discriminative power. With ~1-2% adoption rates, pushing most
   predictions low is correct behavior.

8. RANK AVERAGING >>> ARITHMETIC BLENDING for rare events. Arithmetic
   mean pulls predictions toward 0.5 and destroys calibration. Rank
   averaging preserves the distribution shape while using consensus ordering.
   This single insight was worth +0.003 at same blend ratios.

9. Distribution mapping matters. Blending sorted value distributions in
   logit space (RANKLOGODDS) is superior to mapping to a single source's
   distribution, especially for extreme class imbalance.

10. Blend ALL metric components. We initially only blended LL columns (75%
    of metric) and left AUC untouched. Adding even 15% V4 to AUC columns
    gave +0.000817 — the AUC component has independent diversity gains too.

11. Post-prediction blending can outperform model improvements. Our best
    single model scored 0.942924. Blending techniques alone added +0.007712
    without changing any model. This is a bigger gain than any model change.


FILES IN THIS REPOSITORY
--------------------------
Best submission:
  - sub_BEST2_rklo80d60_auc15.csv  : LB 0.950636 *** CURRENT BEST ***
    (RKLO r80 d60 on LL + 15% rank-avg AUC blending)

Source files for blending:
  - sub_ADV_pertarget_progressive.csv : Our best multi-seed LGB (0.942924)
  - submission_v4_ensemble (3).csv    : Teammate's LGB+CB+XGB ensemble

Blending scripts:
  - gen_best.py              : RKLO + AUC blending experiments
  - gen_best2.py             : Full AUC gradient grid (winning approach)
  - gen_rankavg.py           : Rank-averaging blend generator
  - jackpot_blend.py         : 6 blending strategies (rank-avg, geomean, etc.)
  - blend_v4ens3.py          : Original LL-only + full blends
  - blend_v4ens3_finetune.py : Fine-grid LL-only blends

Model pipelines:
  - CompetitiveSolution.py   : Original pipeline (LB 0.91788)
  - ULTIMATE_V4.py           : 20-seed LGB pipeline
  - ULTIMATE_V5.py           : LGB+XGB+Stack pipeline
  - ULTIMATE_V6_FAST.py      : CatBoost+LGB pipeline (did not complete)

Analysis:
  - cross_analysis.py        : Diagnostic analysis of Prior/Train/Test
  - check_new_data.py        : Schema profiling for new dataset
  - DEEP_EDA_ANALYSIS.py     : Deep EDA

Data:
  - Train.csv, Test.csv, Prior.csv
  - SampleSubmission.csv, dataset_data_dictionary.csv

Documentation:
  - SOLUTION_EVOLUTION.txt   : Full score progression and strategy details
  - SOLUTION_EXPLANATION.txt : This file
