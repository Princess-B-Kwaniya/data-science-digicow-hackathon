================================================================================
DIGICOW HACKATHON — SOLUTION EVOLUTION & WHAT WORKED
================================================================================
Team: Princess-B-Kwaniya
Competition: Zindi DigiCow — Predicting Dairy Technology Adoption
Metric: Per-target 0.75*LogLoss + 0.25*(1-AUC), summed across 3 targets
        HIGHER leaderboard score = BETTER
Date: February 2026

================================================================================
⚠️  DATASET REFRESH — January/February 2026
================================================================================
Zindi completely replaced the dataset mid-competition. All old submissions and
scores from the first phase (best: 0.72828) are INVALIDATED. New data has:
  - Different schema: farmer_name (was farmer_id), training_day (was training_date)
  - topics_list is now nested list-of-lists (was flat list)
  - trainer is now list format (was plain string)
  - New file: Prior.csv (44,882 rows) — farmer history from earlier sessions
  - Train: 13,536 rows, Test: 5,621 rows
  - Target rates: 7d=1.13%, 90d=1.58%, 120d=2.23% (much lower than before)
  - Zero farmer overlap between Train and Test
  - 62.7% of test farmers appear in Prior.csv (3,526/5,621)

================================================================================
CURRENT BEST SCORE: 0.91788 (sub_COMP_D_dual_prior_blend.csv)
================================================================================

APPROACH: LightGBM (Train-only) + Prior.csv as FEATURE source + DUAL strategy
          + Prior rate blending + zero-group post-processing

================================================================================
SCORE PROGRESSION — PHASE 2 (New Data)
================================================================================

Version     | LB Score  | Strategy                              | Status
------------|-----------|---------------------------------------|--------
COMP_A      | 0.91748   | LightGBM 205 features, Prior as feats | Good
COMP_B      | ~0.9175   | DUAL (rank AUC + calibrated LL)       | Similar
COMP_D      | 0.91788   | DUAL + 70/30 Prior rate blend          | BEST ★
ENS_G       | 0.90705   | 3-model ens, trained on Prior+Train   | WORSE ✗
REFv3_A     | 0.88230   | 3-model ens, new features, Train-only | WORSE ✗
REFv3_D     | 0.91254   | 60% D + 40% REFv3_A blend             | WORSE ✗

PHASE 1 (Old Data — now invalidated):
V12/DUAL    | 0.72828   | Bayesian hierarchical + DUAL + rules  | Old best

================================================================================
WHAT WORKED (Phase 2)
================================================================================

1. PRIOR.CSV AS FEATURE SOURCE (GAME CHANGER)
   ────────────────────────────────────────────
   Prior.csv has 44,882 rows of farmer history. 62.7% of test farmers appear
   in it with an average of 8.1 prior training sessions each. We extract:
   
   FARMER HISTORY (19 features):
     prior_session_count, prior_07/90/120_rate, prior_07/90/120_adopted,
     prior_has_topic_rate, prior_coop_rate, prior_unique_groups/wards/trainers,
     prior_unique_topics, prior_training_span_days, prior_ever_adopted_07/90/120,
     prior_any_adoption, prior_adoption_score
   
   GROUP HISTORY (9 features):
     prior_grp_size, prior_grp_07/90/120_rate, prior_grp_has_topic_rate,
     prior_grp_coop_rate, prior_grp_unique_farmers/trainers, prior_grp_adoption_score
   
   GEO HISTORY (18 features across ward/subcounty/county):
     prior_{geo}_size, prior_{geo}_07/90/120_rate,
     prior_{geo}_coop_rate, prior_{geo}_has_topic_rate
   
   PRIOR TARGET ENCODING (12 features):
     prior_te_{group/ward/subcounty/county}_{target} with smoothing=20
   
   WHY: Pre-existing farmer behavior is the strongest predictor of future
   behavior. Prior adoption rates at farmer, group, and geo levels encode
   the fundamental propensity to adopt technology.

2. TRAIN-ONLY MODELING (CRITICAL)
   ────────────────────────────────
   We train LightGBM ONLY on Train.csv (13,536 rows). Prior.csv is used
   exclusively as a feature source — NEVER as training data.
   
   WHY: Prior has different distributions (higher adoption rates: 1.5-4.7%
   vs Train's 1.1-2.2%). Training on Prior+Train (58K rows) shifts
   predictions upward, destroying LogLoss calibration. This was confirmed
   by ENS_G (0.907) which trained on combined data.

3. DUAL COLUMN STRATEGY
   ──────────────────────
   Same principle as Phase 1: separate AUC and LogLoss columns.
   - AUC columns: rank-based (percentile * 0.998 + 0.001)
   - LL columns: calibrated model predictions
   
   Impact: COMP_A (standard) → COMP_D (DUAL) = ~0.0004 improvement

4. PRIOR RATE BLENDING (70/30)
   ─────────────────────────────
   For test farmers with prior history (62.7%), blend:
     final = 0.7 * model_pred + 0.3 * prior_adoption_rate
   
   This anchors predictions toward known farmer behavior.
   Impact: COMP_B (DUAL only) → COMP_D (DUAL + blend) = small improvement

5. ZERO-GROUP RULES (same principle as Phase 1)
   ──────────────────────────────────────────────
   - has_topic_trained_on=0 → force 0.001 (24 test rows)
   - Groups with 30+ train samples and 0% adoption → cap at 0.005
     (120 groups for 7d, 110 for 90d, 104 for 120d)

================================================================================
WHAT FAILED (Phase 2 — critical lessons)
================================================================================

1. TRAINING ON PRIOR+TRAIN COMBINED (ENS_G = 0.907)
   Prior has higher adoption rates and different distributions.
   Mixing it with Train inflated predictions (LogLoss means: 0.031-0.049
   vs D's 0.021-0.034). The models learned Prior's distribution, not Test's.
   LESSON: Prior is FEATURES only, never training data.

2. ADDING NEW FEATURES (REFv3_A = 0.882, MASSIVE DROP)
   REFINED_v3.py added train_grp_*_rate features (train group adoption rates
   computed from full training set — TARGET LEAKAGE) plus other new features
   like prior_sessions_log, farmer_recency_weight, weighted rates, etc.
   Despite better CV (2.945 vs 2.903), LB dropped from 0.918 to 0.882!
   
   The train_grp_*_rate features are TARGET LEAKAGE: they use the full
   training set's target values as features, which overfits the model to
   training patterns that don't generalize. Even though they were the
   top-ranked features by importance (train_grp_07_rate: importance 234),
   they HURT generalization.
   
   LESSON: More features ≠ better. Additional features can overfit CV
   while destroying test performance. Trust the PROVEN feature set.

3. BLENDING WITH BAD PREDICTIONS (REFv3_D = 0.913)
   Even a conservative 60% D + 40% REFv3_A blend dragged D down from
   0.918 to 0.913. The new ensemble predictions were so poor that even
   minority blending poisoned the submission.
   LESSON: Only blend predictions of SIMILAR or BETTER quality.

4. PLATT SCALING
   Platt only helped 7d target (LL 0.021→0.021), made 90d/120d worse.
   The base model's predictions are already well-calibrated.

5. 3-MODEL ENSEMBLE (different seeds/params)
   The ensemble of 3 LightGBM configs (v1/v2/v3 with weights 0.60/0.25/0.15)
   showed marginal CV improvement (2.945 vs 2.942 for best single model)
   but combined with new features, the overall pipeline was WORSE.

================================================================================
KEY FILES
================================================================================

CURRENT BEST SUBMISSION: sub_COMP_D_dual_prior_blend.csv (LB 0.91788)
  Generated by: CompetitiveSolution.py
  Pipeline: LightGBM (Train-only) + Prior features + DUAL + Prior blend
  205 features, 5-fold StratifiedKFold, scale_pos_weight for imbalance
  CV: 7d AUC=0.975/LL=0.033, 90d AUC=0.977/LL=0.035, 120d=0.981/LL=0.039

PHASE 2 FILES:
  CompetitiveSolution.py  — Produced BEST submission (0.91788)
  ENHANCED_V2.py          — Prior+Train mega-training (FAILED: 0.907)
  REFINED_v3.py           — 3-model ensemble + new features (FAILED: 0.882)
  cross_analysis.py       — Prior/Train/Test diagnostic analysis
  check_new_data.py       — Schema profiling for new data

PHASE 1 FILES (old data, scores invalidated):
  GROUPS_V3.py, GROUPS_V3_FAST.py, GROUPS_V4.py — Bayesian hierarchies
  DUAL_FINAL.py — DUAL strategy + zero-group rules
  SIMPLE_SOLUTION.py — Simple baseline
  DEEP_LGBM.py, DIVERSITY_BLEND.py — Various attempts

DATA FILES:
  Train.csv (13,536 rows), Test.csv (5,621 rows), Prior.csv (44,882 rows)
  SampleSubmission.csv, dataset_data_dictionary.csv

================================================================================
FINAL ARCHITECTURE (Current Best — CompetitiveSolution.py)
================================================================================

  Prior.csv ──→ Feature Extraction ──→ 46 farmer/group/geo history features
                                       12 prior target-encoded features
                                            │
  Train.csv ──┐                             │
              ├──→ Feature Engineering ──────┼──→ LightGBM (5-fold CV)
  Test.csv  ──┘    (205 total features)     │    ↓
                   - Temporal (16)                │    OOF predictions
                   - Topic categories (15)        │    ↓
                   - Frequency encoding (11)      ├──→ DUAL Strategy ──→ Post-Processing
                   - Group features (9)           │    (rank AUC +      (topic=0→0.001,
                   - Trainer features (5)         │     calibrated LL)   zero-group→0.005)
                   - Demographics (9)             │         ↓
                   - Interactions (5)             │    Prior Rate Blend
                   - Target encoding OOF (45)     │    (70% model + 30% prior rate
                   - History interactions (7)     │     for farmers with history)
                   - Aggregations (9)             │         ↓
                                                  │    Submission
                                                  │    (sub_COMP_D = LB 0.91788)

================================================================================
NEXT APPROACH (with limited submissions remaining)
================================================================================

The key insight: CompetitiveSolution.py with its EXACT 205 features is proven.
Any new features HURT (train_grp_rate leakage, recency features = noise).

SAFE STRATEGIES TO TRY:
  1. RE-RUN CompetitiveSolution.py with ONLY seed diversity (different random
     seeds for the same model/features), average the predictions.
     This adds variance reduction without any new features or leakage risk.
  
  2. Tune the Prior blend ratio (try 60/40 or 80/20 instead of 70/30).
     Small change, low risk.
  
  3. Soften zero-group thresholds (try cap at 0.003 instead of 0.005).
  
  4. Try different LightGBM hyperparameters WITHIN the same feature set
     (e.g., num_leaves=47 or 31 for more regularization).

AVOID:
  - Adding ANY new features (proven to hurt)
  - Training on Prior.csv data
  - Large ensemble blends with unproven models
  - Platt/isotonic calibration (marginal at best)

================================================================================
